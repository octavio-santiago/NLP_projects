# -*- coding: utf-8 -*-
"""NLP.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1HF-Duwcnjk59B-fzEltA52xxHWURAIfO
"""

import sklearn
import pandas as pd
import re

from sklearn.model_selection import cross_val_score, GridSearchCV, KFold, RandomizedSearchCV, train_test_split 
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.svm import LinearSVC
from sklearn.metrics import accuracy_score

import nltk
nltk.download('stopwords')
nltk.download('wordnet')
from nltk.corpus import stopwords

# Commented out IPython magic to ensure Python compatibility.
'''from google.colab import drive
drive.mount('/gdrive')'''
# %cd /gdrive

#https://towardsdatascience.com/sentiment-analysis-with-python-part-1-5ce197074184
path = "/gdrive/My Drive/Colab Notebooks/comments/"
filename = 'comments ed sherran dont.csv'
#filename = 'comments-mensinou bateria.csv'
df = pd.read_csv('datasets/'+filename)
df.head()

df = df.dropna(subset=['user'])
df = df.dropna(subset=['commentText'])
len(df)

df = df[['user','commentText','likes']]
df.head()

def label(x):
  if x > 1000:
    return "the best"
  elif x > 100:
    return 'nice'
  elif x > 10:
    return 'good'
  else:
    return 'bad'
  
  
df['label'] = df.likes.apply(lambda x: 'like' if x > 100 else 'not like')
#df['label'] = df.likes.apply(lambda x: label(x))
df.head()

train = df[['commentText','label']]
X = train.drop(['label'],axis=1)
y = train['label'].values

#clean
REPLACE_NO_SPACE = re.compile("[.;:!\'?,\"()\[\]]")
REPLACE_WITH_SPACE = re.compile("(<br\s*/><br\s*/>)|(\-)|(\/)#%+")

def preprocess_comments(df):
    df.commentText = df.commentText.apply(lambda x: REPLACE_NO_SPACE.sub("", str(x)))
    df.commentText = df.commentText.apply(lambda x: REPLACE_WITH_SPACE.sub("", str(x)))
    df.commentText = df.commentText.apply(lambda x: x.lower())
    df.commentText = df.commentText.apply(lambda x: re.sub(r"[%+123456789020]",'',str(x)))
    #df.commentText = df.commentText.apply(lambda x: re.sub(r"(<br\s*/><br\s*/>)|(\-)|(\/)#",'', str(x)))
    
    return df

X = preprocess_comments(X)

print(X.head())

X = X.values.tolist()
X = [re.sub(r"[[]",'',str(p)) for p in X]
X = [re.sub(r"[]]",'',str(p)) for p in X]

#Removing Stop Words
english_stop_words = stopwords.words('english')
#english_stop_words = stopwords.words('portuguese')

def remove_stop_words(corpus):
    removed_stop_words = []
    for review in corpus:
        removed_stop_words.append(
            ' '.join([word for word in review.split() 
                      if word not in english_stop_words])
        )
    return removed_stop_words

X = remove_stop_words(X)

#Normalization

def get_stemmed_text(corpus):
    from nltk.stem.porter import PorterStemmer
    stemmer = PorterStemmer()
    return [' '.join([stemmer.stem(word) for word in review.split()]) for review in corpus]

#X = get_stemmed_text(X)

def get_lemmatized_text(corpus):
    from nltk.stem import WordNetLemmatizer
    lemmatizer = WordNetLemmatizer()
    return [' '.join([lemmatizer.lemmatize(word) for word in review.split()]) for review in corpus]

#X = get_lemmatized_text(X)

#Vectorization

def vectorizer(df):
  cv = CountVectorizer(binary=True)
  cv.fit(df)
  df = cv.transform(df)
  return df,cv

#n-grams
def ngram_vectorizer(df):
  ngram_vectorizer = CountVectorizer(binary=True, ngram_range=(1, 2))
  ngram_vectorizer.fit(df)
  df = ngram_vectorizer.transform(df)
  return df,ngram_vectorizer

def word_count_vectorizer(df):
  wc_vectorizer = CountVectorizer(binary=False)
  wc_vectorizer.fit(df)
  df = wc_vectorizer.transform(df)
  return df,wc_vectorizer

# TD-IDF
def tf_idf_vectorizer(df):
  tfidf_vectorizer = TfidfVectorizer()
  tfidf_vectorizer.fit(df)
  df = tfidf_vectorizer.transform(df)
  return df,tf_idf_vectorizer

X,vec = vectorizer(X) #0.958139534883721
#X,vec = ngram_vectorizer(X) #0.9565
#X,vec = word_count_vectorizer(df) 
#X,vec = tf_idf_vectorizer(X) #0.958139534883721

#y = y.values
#y = y.reshape((1,len(y)))
#y.shape

#separar 70/30 treino e teste
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)

# Commented out IPython magic to ensure Python compatibility.
#model
#X_train = X_train.reshape((len(X_train),1))

def train_model(X_train,y_train,X_test,y_test):
  models = []
  acc_models = []
  print('Logistic Regression')
  for c in [0.01, 0.05, 0.25, 0.5, 1]:

      lr = LogisticRegression(C=c)
      lr.fit(X_train, y_train)
      acc = accuracy_score(y_test, lr.predict(X_test))
      models.append(lr)
      acc_models.append(acc)
      
      print ("Accuracy for C=%s: %s"% (c, acc))
      
  print("")   
  print('SVC')
  for c in [0.01, 0.05, 0.25, 0.5, 1]:
    
    svm = LinearSVC(C=c)
    svm.fit(X_train, y_train)
    models.append(svm)
    acc = accuracy_score(y_test, svm.predict(X_test))
    acc_models.append(acc)
    
    print ("Accuracy for C=%s: %s"% (c, acc))
  
  
  
  idx = acc_models.index(max(acc_models))
  final_model = models[idx]
  return final_model

final_model = train_model(X_train,y_train,X_test,y_test)

# Commented out IPython magic to ensure Python compatibility.
#final_model = LogisticRegression(C=0.01)
final_model.fit(X_train, y_train)
print ("Final Accuracy: %s"% accuracy_score(y_test, final_model.predict(X_test)))

feature_to_coef = {
    word: coef for word, coef in zip(
        vec.get_feature_names(), final_model.coef_[0]
    )
}

for best_positive in sorted(
    feature_to_coef.items(), 
    key=lambda x: x[1], 
    reverse=True)[:5]:
    print (best_positive)

for best_negative in sorted(
    feature_to_coef.items(), 
    key=lambda x: x[1])[:5]:
    print (best_negative)

t = df[df['commentText'].str.contains("love")].sort_values(by=['likes'],ascending=False)
#t = t[t['likes'] > 10]
t.head()

print(t.iloc[0,1])

print(df.sort_values(by=['likes'],ascending=False)[['commentText','label']].head())

comment = ['I literally live for this video','HOW CAN HE GET ENOUGH BREATH????????','fjytru ft grt','I wish i could spend 1 hour for singing lesson']
comment = [REPLACE_NO_SPACE.sub("", str(x)) for x in comment]
comment = [REPLACE_WITH_SPACE.sub("", str(x)) for x in comment]
comment = [x.lower() for x in comment]
comment = [re.sub(r"[%+123456789020]",'',str(x)) for x in comment]
comment = vec.transform(comment)
#comment = vec.transform(comment).todense()
#print(vec.inverse_transform(comment))
#comment,cv2 = vectorizer(comment)
print(final_model.predict(comment))
print(final_model.predict_proba(comment))


